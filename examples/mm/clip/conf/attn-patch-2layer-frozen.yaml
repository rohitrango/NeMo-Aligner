# An example model that works with this config is "https://huggingface.co/yuvalkirstain/PickScore_v1"
defaults:
  - megatron_multicrop_rm_transformer

name: megatron_multicrop_rm_attn_patch_2layer_frozen

trainer:
  max_steps: 20000
  val_check_interval: 100
  precision: 32

model:
  aggregator: attn_patch
  micro_batch_size: 32
  image_patch_attn:
    num_layers: 2
    num_attention_heads: 8
    num_crops_grad: 1
    add_global_image_context: False    # DO NOT add global context to the full image in `attn_patch` setting (subject to change)
  vision:
    freeze: True
    post_process: False
  text:
    freeze: True
    post_process: False  # important to avoid post-processing the clip and vit features
  data:
    num_workers: 8
  optim:
    lr: 1e-5        # reduced lr for transformer seems to work better? 
    sched:
      name: PolynomialDecayAnnealing
      warmup_steps: 500
      power: 1.0
      min_lr: 0
