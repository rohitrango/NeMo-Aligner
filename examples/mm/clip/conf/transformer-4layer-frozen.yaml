# An example model that works with this config is "https://huggingface.co/yuvalkirstain/PickScore_v1"
defaults:
  - megatron_multicrop_rm_transformer

name: megatron_multicrop_rm_transformer_4layer_frozen

trainer:
  max_steps: 25000
  val_check_interval: 200
  precision: 32

model:
  micro_batch_size: 16
  image_patch_attn:
    num_layers: 4
    num_attention_heads: 8
    num_crops_grad: 1
    add_global_image_context: True    # add global context to the full image
  vision:
    freeze: True
  text:
    freeze: True
  data:
    num_workers: 8
  optim:
    lr: 1e-5        # reduced lr for transformer seems to work better? 
    sched:
      name: PolynomialDecayAnnealing
      warmup_steps: 500
      power: 1.0
      min_lr: 0
